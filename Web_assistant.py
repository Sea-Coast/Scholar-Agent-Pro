import streamlit as st
import os
import json
import time
import threading
import asyncio
import sys
import hashlib
import shutil
import re
import random
import httpx
import pandas as pd
from datetime import datetime
from openai import OpenAI
from playwright.async_api import async_playwright

# === Windows å¹³å°å¼‚æ­¥ä¿®å¤ ===
if sys.platform.startswith("win"):
    asyncio.set_event_loop_policy(asyncio.WindowsProactorEventLoopPolicy())

# ================= 1. å…¨å±€é…ç½®ä¸çŠ¶æ€ç®¡ç† =================

CONFIG_FILE = "web_config.json"
HISTORY_DB = "history_map.json"
DEFAULT_CONFIG = {
    "api_key": "sk-xxxx",
    "base_url": "https://api.openai.com/v1",
    "model_name": "gpt-4o-mini",
    "proxy_url": "http://127.0.0.1:7897",
    "watch_dir": "./incoming",
    "library_dir": "./MyLibrary"
}

# åˆå§‹åŒ– Session State
if "logs" not in st.session_state: st.session_state.logs = []
if "monitor_running" not in st.session_state: st.session_state.monitor_running = False
if "thread_obj" not in st.session_state: st.session_state.thread_obj = None
if "stop_event" not in st.session_state: st.session_state.stop_event = threading.Event()
if "history_records" not in st.session_state: st.session_state.history_records = []


def load_config():
    if os.path.exists(CONFIG_FILE):
        try:
            with open(CONFIG_FILE, "r", encoding="utf-8") as f:
                return {**DEFAULT_CONFIG, **json.load(f)}
        except:
            pass
    return DEFAULT_CONFIG


def save_config(config):
    with open(CONFIG_FILE, "w", encoding="utf-8") as f:
        json.dump(config, f, indent=4)


def add_log(message):
    timestamp = datetime.now().strftime("%H:%M:%S")
    st.session_state.logs.insert(0, f"{timestamp} | {message}")
    if len(st.session_state.logs) > 100: st.session_state.logs.pop()


def add_history_record(filename, folder, summary):
    st.session_state.history_records.insert(0, {
        "æ—¶é—´": datetime.now().strftime("%H:%M"),
        "æ–‡ä»¶å": filename,
        "AIå½’æ¡£åˆ†ç±»": folder,
        "æ‘˜è¦é¢„è§ˆ": summary[:30] + "..." if summary else "æ— æ‘˜è¦"
    })


# ================= 2. åå°é€»è¾‘æ ¸å¿ƒ (æ— å˜åŠ¨) =================

class BackendLogic:
    def __init__(self, config):
        self.config = config
        os.environ["HTTP_PROXY"] = self.config["proxy_url"]
        os.environ["HTTPS_PROXY"] = self.config["proxy_url"]
        if not os.path.exists(self.config["watch_dir"]): os.makedirs(self.config["watch_dir"])
        if not os.path.exists(self.config["library_dir"]): os.makedirs(self.config["library_dir"])

    def _get_md5(self, path):
        hash_md5 = hashlib.md5()
        try:
            with open(path, "rb") as f:
                for chunk in iter(lambda: f.read(4096), b""): hash_md5.update(chunk)
            return hash_md5.hexdigest()
        except:
            return None

    def _ai_analyze_full(self, content):
        client = OpenAI(api_key=self.config["api_key"], base_url=self.config["base_url"])
        today = datetime.now().strftime('%Y-%m-%d')
        existing = []
        if os.path.exists(self.config["library_dir"]):
            existing = [f for f in os.listdir(self.config["library_dir"]) if f.startswith(today)]

        system_prompt = "ä½ æ˜¯ä¸€ä¸ªç§‘ç ”åŠ©ç†ã€‚åˆ†ææ–‡çŒ®å¹¶è¾“å‡ºJSONã€‚"
        user_prompt = f"""
        ä»»åŠ¡ï¼š1.å†³å®šå½’æ¡£æ–‡ä»¶å¤¹å(å±äº{existing}åˆ™å¤ç”¨ï¼Œå¦åˆ™ç”Ÿæˆ'{today}'+'15å­—ä¸»é¢˜')ã€‚2.ç”Ÿæˆ200å­—æ‘˜è¦ã€‚
        å†…å®¹: {content}
        è¿”å›JSON: {{"folder_name": "...", "summary": "..."}}
        """
        try:
            resp = client.chat.completions.create(
                model=self.config["model_name"],
                messages=[{"role": "system", "content": system_prompt}, {"role": "user", "content": user_prompt}],
                temperature=0.3,
                response_format={"type": "json_object"}
            )
            result = json.loads(resp.choices[0].message.content)
            f_name = re.sub(r'[\\/:*?"<>|]', '', result.get("folder_name", f"{today}_æœªåˆ†ç±»"))
            return f_name, result.get("summary", "")
        except Exception as e:
            add_log(f"AI Error: {e}")
            return f"{today}_AIå¤±è´¥", ""

    def process_single_file(self, f_path, status_container=None):
        if not os.path.exists(f_path): return
        filename = os.path.basename(f_path)

        if status_container: status_container.write(f"ğŸ“¦ æ£€æµ‹åˆ°æ–‡ä»¶: {filename}")
        add_log(f"ğŸ“¦ å¼€å§‹å¤„ç†: {filename}")

        md5 = self._get_md5(f_path)
        history = {}
        if os.path.exists(HISTORY_DB):
            try:
                with open(HISTORY_DB, 'r', encoding='utf-8') as db:
                    history = json.load(db)
            except:
                pass

        target_folder, summary = "", ""
        if md5 in history:
            target_folder = history[md5]
            if status_container: status_container.info(f"âš¡ å‘½ä¸­å†å²è®°å½•ï¼Œè·³è¿‡AIåˆ†æ")
            add_log(f"âš¡ è®°å¿†å‘½ä¸­: {target_folder}")
        else:
            if status_container: status_container.write("ğŸ§  æ­£åœ¨æå–æ–‡æœ¬å¹¶è¿›è¡ŒAIåˆ†æ...")
            import fitz
            text = ""
            try:
                with fitz.open(f_path) as doc:
                    for p in doc:
                        text += p.get_text()
                        if len(text) > 2000: break
            except:
                pass
            target_folder, summary = self._ai_analyze_full(text[:2000])
            add_log(f"ğŸ§  AIå†³ç­–: {target_folder}")
            history[md5] = target_folder
            with open(HISTORY_DB, 'w', encoding='utf-8') as db:
                json.dump(history, db, indent=2, ensure_ascii=False)

        full_target = os.path.join(self.config["library_dir"], target_folder)
        if not os.path.exists(full_target): os.makedirs(full_target)

        if summary:
            with open(os.path.join(full_target, "readme.txt"), "a", encoding="utf-8") as f:
                f.write(f"ğŸ“„ {filename}\nğŸ•’ {datetime.now().strftime('%Y-%m-%d %H:%M')}\nğŸ“ {summary}\n{'-' * 50}\n\n")

        dest = os.path.join(full_target, filename)
        if os.path.exists(dest): dest = os.path.join(full_target, f"copy_{int(time.time())}_{filename}")

        try:
            shutil.move(f_path, dest)
            add_log(f"âœ… å½’æ¡£æˆåŠŸ: {target_folder}")
            add_history_record(filename, target_folder, summary)
            if status_container: status_container.success(f"å½’æ¡£å®Œæˆï¼å­˜å…¥: {target_folder}")
        except Exception as e:
            add_log(f"âŒ ç§»åŠ¨å¤±è´¥: {e}")

    def monitor_process(self, stop_event):
        add_log("ğŸŸ¢ ç›‘æ§çº¿ç¨‹å·²å¯åŠ¨")
        while not stop_event.is_set():
            try:
                files = [f for f in os.listdir(self.config["watch_dir"]) if f.lower().endswith('.pdf')]
                for f in files:
                    if stop_event.is_set(): break
                    self.process_single_file(os.path.join(self.config["watch_dir"], f))
                time.sleep(2)
            except:
                time.sleep(2)
        add_log("ğŸ”´ ç›‘æ§çº¿ç¨‹å·²åœæ­¢")

    async def _smart_scroll(self, page, status_container=None):
        if status_container: status_container.write("ğŸ“œ æ­£åœ¨æ™ºèƒ½æ»šåŠ¨åŠ è½½é•¿å›¾æ–‡...")
        try:
            viewport_height = await page.evaluate("window.innerHeight")
            current_scroll = 0
            while True:
                doc_height = await page.evaluate("document.body.scrollHeight")
                current_scroll += (viewport_height - 100)
                await page.evaluate(f"window.scrollTo(0, {current_scroll})")
                await asyncio.sleep(random.uniform(1.0, 1.5))
                if current_scroll >= doc_height: break
            await page.evaluate(
                """() => { return Promise.all(Array.from(document.images).filter(img => !img.complete).map(img => new Promise(resolve => { img.onload = img.onerror = resolve; }))); }""")
        except:
            pass

    async def download_link_and_process(self, url, status_container):
        add_log(f"ğŸŒ æ”¶åˆ°ä»»åŠ¡: {url}")
        target_file = None

        if "arxiv.org" in url or url.lower().endswith(".pdf"):
            status_container.write("â¬‡ï¸ æ£€æµ‹åˆ° PDF/ArXivï¼Œå¼€å§‹é«˜é€Ÿä¸‹è½½...")
            if "/abs/" in url: url = url.replace("/abs/", "/pdf/")
            if not url.endswith(".pdf"): url += ".pdf"
            headers = {
                "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"}
            async with httpx.AsyncClient(verify=False, trust_env=True, follow_redirects=True,
                                         headers=headers) as client:
                try:
                    async with client.stream("GET", url, timeout=60.0) as response:
                        if response.status_code == 200:
                            fname = f"download_{int(time.time())}.pdf"
                            target_file = os.path.join(self.config["watch_dir"], fname)
                            with open(target_file, "wb") as f:
                                async for chunk in response.aiter_bytes(): f.write(chunk)
                            add_log("âœ… ä¸‹è½½æˆåŠŸ")
                        else:
                            add_log(f"âŒ ä¸‹è½½å¤±è´¥: {response.status_code}")
                except Exception as e:
                    add_log(f"âŒ ä¸‹è½½å¼‚å¸¸: {e}")
        else:
            status_container.write("ğŸ“¸ æ£€æµ‹åˆ°ç½‘é¡µï¼Œå¯åŠ¨æµè§ˆå™¨æˆªå›¾æ¨¡å¼...")
            async with async_playwright() as p:
                browser = await p.chromium.launch(headless=True)
                context = await browser.new_context(
                    user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36")
                try:
                    page = await context.new_page()
                    await page.goto(url, timeout=90000, wait_until="domcontentloaded")
                    title = await page.title()
                    safe_title = re.sub(r'[\\/:*?"<>|]', '', title).strip() or "webpage"
                    await self._smart_scroll(page, status_container)
                    fname = f"{safe_title}_{int(time.time())}.pdf"
                    target_file = os.path.join(self.config["watch_dir"], fname)
                    await page.emulate_media(media="screen")
                    await page.pdf(path=target_file, format="A4", print_background=True,
                                   margin={"top": "1cm", "bottom": "1cm", "left": "1cm", "right": "1cm"})
                    add_log(f"âœ… è½¬æ¢å®Œæˆ")
                except Exception as e:
                    add_log(f"âŒ ç½‘é¡µå¤±è´¥: {e}")
                finally:
                    await browser.close()

        if target_file and os.path.exists(target_file):
            status_container.write("âš¡ ä¸‹è½½å®Œæ¯•ï¼Œå¼€å§‹AIå½’æ¡£...")
            self.process_single_file(target_file, status_container)


# ================= 3. Streamlit å‰ç«¯ç•Œé¢ (åŸç”Ÿæ¸…çˆ½ç‰ˆ) =================

st.set_page_config(
    page_title="Scholar Agent Pro",
    page_icon="ğŸ“",
    layout="wide",
    initial_sidebar_state="expanded"
)

# è¿™é‡Œä¸åŠ ä»»ä½•å¼ºåˆ¶æ”¹è‰²çš„ CSSï¼Œåªä¿ç•™æœ€åŸºç¡€çš„
# è¿™æ · Streamlit ä¼šè‡ªåŠ¨é€‚é…ä½ çš„ç³»ç»Ÿï¼ˆç™½è‰²/é»‘è‰²æ¨¡å¼ï¼‰

config = load_config()

# --- ä¾§è¾¹æ  ---
with st.sidebar:
    st.header("âš™ï¸ æ§åˆ¶é¢æ¿")
    with st.expander("ğŸ”‘ API é…ç½®", expanded=True):
        new_api_key = st.text_input("API Key", value=config["api_key"], type="password")
        new_base_url = st.text_input("Base URL", value=config["base_url"])
        new_model = st.text_input("Model", value=config["model_name"])

    with st.expander("ğŸŒ ç½‘ç»œ & è·¯å¾„", expanded=False):
        new_proxy = st.text_input("Proxy", value=config["proxy_url"])
        watch_dir = st.text_input("Monitor", value=config["watch_dir"])
        library_dir = st.text_input("Library", value=config["library_dir"])

    # æŒ‰é’®ä½¿ç”¨é»˜è®¤æ ·å¼ï¼Œä¸å¼ºåˆ¶é¢œè‰²
    if st.button("ğŸ’¾ ä¿å­˜é…ç½®", key="save_cfg", use_container_width=True):
        save_config({"api_key": new_api_key, "base_url": new_base_url, "model_name": new_model, "proxy_url": new_proxy,
                     "watch_dir": watch_dir, "library_dir": library_dir})
        st.success("é…ç½®å·²ä¿å­˜")
        time.sleep(0.5)
        st.rerun()

# --- ä¸»æ ‡é¢˜ ---
st.title("ğŸ“ Scholar Agent Pro")
st.caption(f"å½“å‰ä»£ç†: `{config['proxy_url']}` | å­˜å‚¨åº“: `{config['library_dir']}`")

# --- æ ¸å¿ƒæ“ä½œåŒº ---
col_input, col_monitor = st.columns([2, 1])

with col_input:
    # ä½¿ç”¨ container å¢åŠ è¾¹æ¡†ï¼Œæå‡å±‚æ¬¡æ„Ÿ
    with st.container(border=True):
        st.subheader("ğŸ“¥ æ–°ä»»åŠ¡")
        url_input = st.text_input("ç²˜è´´é“¾æ¥ (ArXiv / å…¬ä¼—å· / PDF):", label_visibility="collapsed",
                                  placeholder="https://...")

        if st.button("ğŸš€ å¼€å§‹æŠ“å–å¹¶æ•´ç†", type="primary", key="start_btn", use_container_width=True):
            if not url_input:
                st.warning("è¯·å…ˆè¾“å…¥é“¾æ¥")
            else:
                with st.status("æ­£åœ¨å…¨è‡ªåŠ¨å¤„ç†ä¸­...", expanded=True) as status:
                    backend = BackendLogic(config)
                    asyncio.run(backend.download_link_and_process(url_input, status))
                    status.update(label="âœ… ä»»åŠ¡å…¨éƒ¨å®Œæˆ", state="complete", expanded=False)
                st.rerun()

with col_monitor:
    with st.container(border=True):
        st.subheader("ğŸ“¡ åå°ç›‘æ§")
        if st.session_state.monitor_running:
            st.success("ğŸŸ¢ è¿è¡Œä¸­")
            if st.button("â¹ åœæ­¢ç›‘æ§", key="stop_mon", use_container_width=True):
                st.session_state.stop_event.set()
                if st.session_state.thread_obj: st.session_state.thread_obj.join()
                st.session_state.monitor_running = False
                st.rerun()
        else:
            st.info("ğŸ”´ å·²åœæ­¢")
            if st.button("â–¶ å¯åŠ¨ç›‘æ§", key="start_mon", use_container_width=True):
                st.session_state.stop_event.clear()
                backend = BackendLogic(config)
                t = threading.Thread(target=backend.monitor_process, args=(st.session_state.stop_event,), daemon=True)
                t.start()
                st.session_state.thread_obj = t
                st.session_state.monitor_running = True
                st.rerun()

st.divider()

# --- å†å²è®°å½•ä¸æ—¥å¿— ---
col_history, col_logs = st.columns([2, 1])

with col_history:
    st.subheader("ğŸ—‚ï¸ ä»Šæ—¥å½’æ¡£è®°å½•")
    if st.session_state.history_records:
        df = pd.DataFrame(st.session_state.history_records)
        st.dataframe(
            df,
            use_container_width=True,
            hide_index=True,
            column_config={
                "æ‘˜è¦é¢„è§ˆ": st.column_config.TextColumn("æ‘˜è¦é¢„è§ˆ", help="é¼ æ ‡æ‚¬åœæŸ¥çœ‹è¯¦æƒ…", width="medium"),
                "AIå½’æ¡£åˆ†ç±»": st.column_config.TextColumn("å½’æ¡£ä½ç½®", width="medium")
            }
        )
    else:
        st.info("ä»Šæ—¥æš‚æ— å½’æ¡£è®°å½•")

with col_logs:
    st.subheader("ğŸ“Ÿ ç³»ç»Ÿæ—¥å¿—")
    # ä½¿ç”¨åŸç”Ÿçš„ text_area æ˜¾ç¤ºæ—¥å¿—ï¼Œæœ€å®‰å…¨æ¸…æ™°
    log_text = "\n".join(st.session_state.logs)
    st.text_area("Log Output", value=log_text, height=300, label_visibility="collapsed", disabled=True)

    if st.button("ğŸ”„ åˆ·æ–°æ—¥å¿—", key="ref_log", use_container_width=True):
        st.rerun()